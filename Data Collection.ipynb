{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import timezone\n",
    "import time\n",
    "import os\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "url_com = 'https://api.pushshift.io/reddit/search/comment'\n",
    "\n",
    "res = requests.get(url, params = {'subreddit' : 'moviesuggestions'})\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function created for Project 3, modified for better functionality and capstone project needs\n",
    "\n",
    "def get_sub_df(sub_name, num_to_pull, start_utc = None, min_comments = 0, drop_removed = True):\n",
    "    #Returns a dataframe containing num_to_pull submissions from the specified subreddit\n",
    "    #Dataframe has ID, Title, selftext and num_comments\n",
    "    \n",
    "    post_count = 0    #Keeps track of how many posts have been pulled\n",
    "    data = []         #Holds all the data to create the DF after the while loop\n",
    "    \n",
    "    res_count = 0     #for debugging\n",
    "    \n",
    "    if start_utc == None:\n",
    "        start_utc = int(datetime.now(timezone.utc).timestamp()) #If the start timestamp isn't specified, use the current time\n",
    "        \n",
    "    min_comment_count = '>' + str(min_comments-1) #paramater to select only submissions with comments greater than the number specified\n",
    "    \n",
    "    timer = 0 \n",
    "    \n",
    "    while post_count < num_to_pull:\n",
    "        #To make sure to get the specific number pulled, check to see if the remaining count is less than 100, if not, just get 100\n",
    "        if (num_to_pull - post_count) < 100:\n",
    "            get_size = (num_to_pull - post_count)\n",
    "        else:\n",
    "            get_size = 100\n",
    "            \n",
    "        timer = time.time() #Timer to avoid 429 codes from making too many requests\n",
    "        \n",
    "        res = requests.get(url, params = {'subreddit' : sub_name, 'size' : get_size, 'before' : start_utc, 'num_comments' : min_comment_count})\n",
    "        res_count += 1\n",
    "        \n",
    "        clear_output(wait=True) #reference: https://stackoverflow.com/a/24818304\n",
    "        \n",
    "        print(f'{100 * post_count/num_to_pull}%')\n",
    "        \n",
    "        # Wait for ONE second, but in 0.1s increments (saves time because the request may have taken >0.1s)\n",
    "        # This shouldn't ever come close to exceeding the limit of 200 requests/minute\n",
    "        while time.time() < (timer + 1):\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        if res.status_code != 200:\n",
    "            print(f'Status Code: {res.status_code}')\n",
    "            print(res_count)\n",
    "            return None\n",
    "            \n",
    "        new_data = res.json()['data']\n",
    "            \n",
    "        data.extend(new_data)\n",
    "        \n",
    "        post_count += 100 #not a problem to go over num_to_pull\n",
    "        \n",
    "        if post_count < num_to_pull:\n",
    "            try:\n",
    "                #This is the starting point for grabbing more posts\n",
    "                start_utc = new_data[-1]['created_utc']\n",
    "            except:\n",
    "                #If that doesn't work, try to find out what went wrong:\n",
    "                print(\"Failed to get UTC of last Post. Printing list element that caused failure.\")\n",
    "                try:\n",
    "                    post_count = num_to_pull\n",
    "                    print(new_data[-1])\n",
    "                except:\n",
    "                    print(\"Printing last list element failed.\")\n",
    "    \n",
    "    print(res_count)\n",
    "    \n",
    "    columns_for_df = ['id', 'title', 'selftext', 'removed_by_category', 'created_utc', 'num_comments', 'link_flair_css_class']\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    #select the columns, but only if they exist in the dataframe - the res won't return data when there are 0 non-null values\n",
    "    df = df[df.columns & columns_for_df]\n",
    "    \n",
    "    #Filter out submissions that have been removed. Could result in small samples for heavily moderated forums, so there is the option to keep them in.\n",
    "    if drop_removed and 'removed_by_category' in df.columns:\n",
    "        df = df[df['removed_by_category'].isna()]\n",
    "    \n",
    "    #Drop this column, it is no longer needed\n",
    "    if 'removed_by_category' in df.columns:\n",
    "        df.drop(columns = 'removed_by_category', inplace = True)\n",
    "    \n",
    "    #Convert timestamps to datetimes\n",
    "    df['created_utc'] = df['created_utc'].apply(datetime.fromtimestamp)    \n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(df):\n",
    "    #function to get dictionary from json data - using submission id numbers from the passed dataframe\n",
    "    #This function should work on any size df*, but may take a looong time if there are a lot of comments to get \n",
    "    # - if new comments are being made on the submissions, this function might not get all the comments\n",
    "    #  * unless there is a limit in the API on how many ids can be passed\n",
    "    \n",
    "    #First, get a list of id's from submissions in the dataframe: \n",
    "    \n",
    "    id_string = ''\n",
    "    \n",
    "    for i in df['id']:\n",
    "        id_string += i + ', ' #needs to be formated like: 'id1234, id1235, id1236'\n",
    "        \n",
    "    id_string = id_string[:-2] #drop the last ', '\n",
    "    \n",
    "    #Get the first batch of data and create the list that will ultimately be returned\n",
    "    com_res = requests.get(url_com, params = {'link_id' : id_string, 'size' : 500})\n",
    "    com_data = com_res.json()['data']\n",
    "    \n",
    "    #Check to make sure that the request worked:\n",
    "    if com_res.status_code != 200:\n",
    "        print(f'Status code: {com_res.status_code}')\n",
    "        return None\n",
    "    \n",
    "    #This is the goal - the number of comments we want to end up with (ideally)\n",
    "    num_comments = df['num_comments'].sum() \n",
    "    \n",
    "    last_length = -1\n",
    "    comment_count = 0\n",
    "    timer = 0\n",
    "    \n",
    "    res_counter = 0\n",
    "    \n",
    "    # Handling the case when there are no comments to get:\n",
    "    if len(com_data) == 0:\n",
    "        return com_data\n",
    "    \n",
    "    while (len(com_data) < num_comments) & (last_length != len(com_data)):\n",
    "        #This makes sure that the while loop doesn't continue forever if something goes wrong\n",
    "        last_length = len(com_data)\n",
    "        \n",
    "        try:\n",
    "            #This is the starting point for grabbing more comments\n",
    "            last_comment_utc = com_data[-1]['created_utc'] \n",
    "        except:\n",
    "            print(\"Failed to get UTC of last comment. Printing list element that caused failure.\")\n",
    "            print(com_data[-1])\n",
    "        \n",
    "        timer = time.time()\n",
    "        \n",
    "        new_res = requests.get(url_com, params = {'link_id' : id_string,\n",
    "                                                  'size' : 500,\n",
    "                                                  'before' : last_comment_utc,\n",
    "                                                  'limit' : 500})\n",
    "        \n",
    "        res_counter += 1\n",
    "        \n",
    "        comment_count += 500\n",
    "        \n",
    "        print(f'{100 * comment_count/num_comments}%')\n",
    "        \n",
    "        \n",
    "        while time.time() < (timer + 1):\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        if new_res.status_code == 200:\n",
    "            \n",
    "            new_data = new_res.json()['data']\n",
    "            \n",
    "            for new_comment in new_data:\n",
    "                com_data.append(new_comment)\n",
    "                \n",
    "        else:\n",
    "            print(f'Problem when pulling new comments: Code {new_res.status_code}')\n",
    "            \n",
    "    print(res_counter)\n",
    "            \n",
    "    return com_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_comments(com_data, df):\n",
    "    #Take comment data ( .json()['data'] ) and make a new column for the dataframe with the comment texts\n",
    "    \n",
    "    df_c = df.copy() #make a copy of the df, just to be safe/explicit. This copy is what is returned by the function.\n",
    "\n",
    "    #Make a zip opject with submission ids and comment text\n",
    "    com_zip = zip([com_data[i]['link_id'][-6:] for i in range(len(com_data))], [com_data[i]['body'] for i in range(len(com_data))])\n",
    "\n",
    "    # create a list of empty lists and make that list the new column\n",
    "    df_c['comments'] = [[] for _ in range(len(df_c))]\n",
    "    \n",
    "    # how many comments are assigned to each row in the dataframe\n",
    "    assignments = np.zeros_like(df_c['num_comments'])\n",
    "\n",
    "    #List of ids, to check if the comment's submission id is in the dataframe\n",
    "    id_list = df_c['id'].values\n",
    "\n",
    "    #counts the total number of comments that can't be assigned to a row\n",
    "    unassigned = 0\n",
    "\n",
    "    #Loop through the zip object, appending comments to the correct row on the 'comments' columns created above, then add 1 to assignment list (to be column later)\n",
    "    for idx, com in com_zip:\n",
    "        if idx in id_list:\n",
    "            df_c[df_c['id'] == idx]['comments'].item().append(com)\n",
    "            assignments[df_c[df_c['id'] == idx].index.item()] += 1\n",
    "        else:\n",
    "            unassigned += 1\n",
    "\n",
    "    if(unassigned > 0):\n",
    "        print(f'There are {unassigned} comments that could not be assigned to a submission!')\n",
    "    \n",
    "    df_c['assigned_comments'] = assignments\n",
    "    \n",
    "    return df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_assign_comments(df):\n",
    "    #This function grabs comments in chunks and assigns them to the submissions\n",
    "    #\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    comment_data = []\n",
    "    \n",
    "    total_length = df_copy.shape[0]\n",
    "    \n",
    "    chunk = 0\n",
    "    chunk_size = 25\n",
    "    \n",
    "    while chunk * chunk_size < total_length:\n",
    "        start_row = chunk * chunk_size\n",
    "        end_row = (chunk + 1) * chunk_size\n",
    "        \n",
    "        if end_row < total_length:\n",
    "            comment_data += get_comments(df_copy[start_row : end_row + 1])\n",
    "        else:\n",
    "            comment_data += get_comments(df_copy[start_row :])\n",
    "            \n",
    "        chunk += 1\n",
    "    \n",
    "    df_copy = assign_comments(comment_data, df_copy)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles_text(sub_name, number = 10_000, start_time = None):\n",
    "    if sub_name + '_data.csv' in os.listdir('./data'):\n",
    "        print(f\"That data ({sub_name}) is already in the data folder. Delete the file if you want to get the data again.\")\n",
    "        return None\n",
    "    else:\n",
    "        df = get_sub_df(sub_name, number, start_utc = start_time)\n",
    "        df = get_and_assign_comments(df)\n",
    "        # -----> Will be moved to new DF and CSV, assignment will be handled differently\n",
    "        df.to_csv('./data/' + sub_name + '_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now(timezone.utc)\n",
    "yesterday = today - timedelta(days = 1)\n",
    "yesterday_timestamp = int(datetime.timestamp(yesterday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0%\n",
      "5\n",
      "82.64462809917356%\n",
      "165.28925619834712%\n",
      "2\n",
      "143.26647564469914%\n",
      "286.5329512893983%\n",
      "2\n",
      "88.65248226950355%\n",
      "177.3049645390071%\n",
      "2\n",
      "70.32348804500704%\n",
      "140.64697609001408%\n",
      "2\n",
      "65.61679790026247%\n",
      "131.23359580052494%\n",
      "2\n",
      "81.16883116883118%\n",
      "162.33766233766235%\n",
      "2\n",
      "91.91176470588235%\n",
      "183.8235294117647%\n",
      "2\n",
      "103.95010395010395%\n",
      "207.9002079002079%\n",
      "2\n",
      "80.64516129032258%\n",
      "161.29032258064515%\n",
      "241.93548387096774%\n",
      "3\n",
      "112.35955056179775%\n",
      "224.7191011235955%\n",
      "2\n",
      "101.01010101010101%\n",
      "202.02020202020202%\n",
      "2\n",
      "118.76484560570071%\n",
      "237.52969121140143%\n",
      "2\n",
      "222.22222222222223%\n",
      "444.44444444444446%\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "get_titles_text('moviesuggestions', number = 500, start_time = yesterday_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/moviesuggestions_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "request    0.861736\n",
       "suggest    0.138264\n",
       "Name: link_flair_css_class, dtype: float64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['link_flair_css_class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>request</th>\n",
       "      <td>252.402985</td>\n",
       "      <td>21.902985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suggest</th>\n",
       "      <td>230.046512</td>\n",
       "      <td>10.209302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           index  num_comments\n",
       "link_flair_css_class                          \n",
       "request               252.402985     21.902985\n",
       "suggest               230.046512     10.209302"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by = 'link_flair_css_class').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'created_utc', 'id', 'link_flair_css_class', 'num_comments',\n",
       "       'selftext', 'title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More work:\n",
    "\n",
    "Currently the csv has all the comments in a single cell.\n",
    "\n",
    "This is a problem because they are saved as one long string, and without any data such as author, score, nest level, etc., which will likely be important as this project goes on. (such as removing negative-score comments from consideration, or collecting data from helpful bots)\n",
    "\n",
    "I will need to create a new database for comments and use that alongside the title/text data. Keeping them together in the same csv won't work for this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
